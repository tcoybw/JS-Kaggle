{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read partition id=0\n",
      "\n",
      "Summary:\n",
      "Total number of partitions read: 1\n",
      "Total number of rows: 1944210\n",
      "Number of columns: 92\n",
      "\n",
      "First few rows of combined data:\n",
      "   date_id  time_id  symbol_id    weight  feature_00  feature_01  feature_02  \\\n",
      "0        0        0          1  3.889038         NaN         NaN         NaN   \n",
      "1        0        0          7  1.370613         NaN         NaN         NaN   \n",
      "2        0        0          9  2.285698         NaN         NaN         NaN   \n",
      "3        0        0         10  0.690606         NaN         NaN         NaN   \n",
      "4        0        0         14  0.440570         NaN         NaN         NaN   \n",
      "\n",
      "   feature_03  feature_04  feature_05  ...  feature_78  responder_0  \\\n",
      "0         NaN         NaN    0.851033  ...   -0.281498     0.738489   \n",
      "1         NaN         NaN    0.676961  ...   -0.302441     2.965889   \n",
      "2         NaN         NaN    1.056285  ...   -0.096792    -0.864488   \n",
      "3         NaN         NaN    1.139366  ...   -0.296244     0.408499   \n",
      "4         NaN         NaN    0.955200  ...    3.418133    -0.373387   \n",
      "\n",
      "   responder_1  responder_2  responder_3  responder_4  responder_5  \\\n",
      "0    -0.069556     1.380875     2.005353     0.186018     1.218368   \n",
      "1     1.190077    -0.523998     3.849921     2.626981     5.000000   \n",
      "2    -0.280303    -0.326697     0.375781     1.271291     0.099793   \n",
      "3     0.223992     2.294888     1.097444     1.225872     1.225376   \n",
      "4    -0.502764    -0.348021    -3.928148    -1.591366    -5.000000   \n",
      "\n",
      "   responder_6  responder_7  responder_8  \n",
      "0     0.775981     0.346999     0.095504  \n",
      "1     0.703665     0.216683     0.778639  \n",
      "2     2.109352     0.670881     0.772828  \n",
      "3     1.114137     0.775199    -1.379516  \n",
      "4    -3.572820    -1.089123    -5.000000  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "def read_partitioned_parquet(base_dir, num_partitions, file_name='part-0.parquet'):\n",
    "    \"\"\"\n",
    "    Read Parquet files from multiple partition directories and combine them into a single DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    base_dir (str): Base directory containing the partition folders\n",
    "    num_partitions (int): Number of partition directories (0 to num_partitions-1)\n",
    "    file_name (str): Name of the Parquet file in each partition directory\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Combined DataFrame from all partition files\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(num_partitions):\n",
    "        # Construct the directory path for each partition\n",
    "        partition_dir = os.path.join(base_dir, f\"partition_id={i}\")\n",
    "        file_path = os.path.join(partition_dir, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Read the Parquet file from the partition\n",
    "            df = pd.read_parquet(file_path)\n",
    "            dfs.append(df)\n",
    "            print(f\"Successfully read partition id={i}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found in partition id={i} at {file_path}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading partition id={i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"No files were successfully read\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Print summary information\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total number of partitions read: {len(dfs)}\")\n",
    "    print(f\"Total number of rows: {len(combined_df)}\")\n",
    "    print(f\"Number of columns: {len(combined_df.columns)}\")\n",
    "    \n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "\n",
    "base_directory = \"C:/Users/oybw/Desktop/Jane Street/jane-street-real-time-market-data-forecasting/train.parquet\"  # Replace with your base directory path\n",
    "num_partitions = 1\n",
    "    \n",
    "# Read all partitions\n",
    "df = read_partitioned_parquet(\n",
    "    base_dir=base_directory,\n",
    "    num_partitions=num_partitions\n",
    "    )\n",
    "    \n",
    "if df is not None:   \n",
    "    # Show first few rows\n",
    "    print(\"\\nFirst few rows of combined data:\")\n",
    "    print(df.head())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check NaN Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing NaN: 1944210\n",
      "\n",
      "NaN count per column:\n",
      "date_id              0\n",
      "time_id              0\n",
      "symbol_id            0\n",
      "weight               0\n",
      "feature_00     1944210\n",
      "                ...   \n",
      "responder_4          0\n",
      "responder_5          0\n",
      "responder_6          0\n",
      "responder_7          0\n",
      "responder_8          0\n",
      "Length: 92, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count rows with any NaN value\n",
    "rows_with_nan = df.isna().any(axis=1).sum()\n",
    "print(f\"Number of rows containing NaN: {rows_with_nan}\")\n",
    "\n",
    "# Or even simpler, to see NaN count per column\n",
    "print(\"\\nNaN count per column:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete NaN Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date_id  time_id  symbol_id    weight  feature_05  feature_06  \\\n",
      "0              0        0          1  3.889038    0.851033    0.242971   \n",
      "1              0        0          7  1.370613    0.676961    0.151984   \n",
      "2              0        0          9  2.285698    1.056285    0.187227   \n",
      "3              0        0         10  0.690606    1.139366    0.273328   \n",
      "4              0        0         14  0.440570    0.955200    0.262404   \n",
      "...          ...      ...        ...       ...         ...         ...   \n",
      "1944205      169      848         19  3.438631   -0.028087    0.287438   \n",
      "1944206      169      848         30  0.768528   -0.022584    0.442352   \n",
      "1944207      169      848         33  1.354696   -0.024804    0.420692   \n",
      "1944208      169      848         34  1.021797   -0.016138    0.303561   \n",
      "1944209      169      848         38  1.570022   -0.017634    0.271368   \n",
      "\n",
      "         feature_07  feature_08  feature_09  feature_10  ...  feature_78  \\\n",
      "0          0.263400   -0.891687          11           7  ...   -0.281498   \n",
      "1          0.192465   -0.521729          11           7  ...   -0.302441   \n",
      "2          0.249901   -0.773050          11           7  ...   -0.096792   \n",
      "3          0.306549   -1.262223          42           5  ...   -0.296244   \n",
      "4          0.344457   -0.613813          44           3  ...    3.418133   \n",
      "...             ...         ...         ...         ...  ...         ...   \n",
      "1944205    0.118074   -0.644495           4           3  ...   -0.166964   \n",
      "1944206    0.140746   -0.571057          81           2  ...   -0.352810   \n",
      "1944207    0.136259   -0.809642          11           7  ...   -0.239716   \n",
      "1944208    0.149970   -0.727993          42           5  ...   -0.442859   \n",
      "1944209    0.128993   -0.611178          50           1  ...   -0.174461   \n",
      "\n",
      "         responder_0  responder_1  responder_2  responder_3  responder_4  \\\n",
      "0           0.738489    -0.069556     1.380875     2.005353     0.186018   \n",
      "1           2.965889     1.190077    -0.523998     3.849921     2.626981   \n",
      "2          -0.864488    -0.280303    -0.326697     0.375781     1.271291   \n",
      "3           0.408499     0.223992     2.294888     1.097444     1.225872   \n",
      "4          -0.373387    -0.502764    -0.348021    -3.928148    -1.591366   \n",
      "...              ...          ...          ...          ...          ...   \n",
      "1944205     0.983339    -0.669860     0.272615    -3.676842    -1.221126   \n",
      "1944206     0.992615     0.961595     1.089402     0.796034     0.488380   \n",
      "1944207     1.701618     0.757672    -5.000000    -3.174266    -1.110790   \n",
      "1944208    -2.036891    -0.064228     1.919665     1.827681     0.872019   \n",
      "1944209     0.323230     0.018376    -3.457667    -0.305218    -0.181438   \n",
      "\n",
      "         responder_5  responder_6  responder_7  responder_8  \n",
      "0           1.218368     0.775981     0.346999     0.095504  \n",
      "1           5.000000     0.703665     0.216683     0.778639  \n",
      "2           0.099793     2.109352     0.670881     0.772828  \n",
      "3           1.225376     1.114137     0.775199    -1.379516  \n",
      "4          -5.000000    -3.572820    -1.089123    -5.000000  \n",
      "...              ...          ...          ...          ...  \n",
      "1944205     1.070584     0.465345     0.207483     0.874975  \n",
      "1944206     1.846634    -0.088542    -0.008324    -0.153451  \n",
      "1944207    -3.349107    -0.407801    -0.185842    -0.931004  \n",
      "1944208     3.248694     0.254584     0.090288     0.434726  \n",
      "1944209    -0.791345     0.347400     0.241875     0.987731  \n",
      "\n",
      "[1944210 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "def drop_nan_columns(df, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Remove columns from DataFrame that contain NaN values based on a threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    threshold (float): Threshold of NaN values (between 0 and 1) above which to drop column\n",
    "                      Default is 1.0 (drop if any NaN exists)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with NaN columns removed\n",
    "    \"\"\"\n",
    "    # Calculate the percentage of NaN values in each column\n",
    "    nan_percent = df.isna().mean()\n",
    "    \n",
    "    # Get columns to keep (those with NaN percentage below threshold)\n",
    "    cols_to_keep = nan_percent[nan_percent < threshold].index\n",
    "    \n",
    "    # Return DataFrame with only those columns\n",
    "    return df[cols_to_keep]\n",
    "\n",
    "# specify a threshold (e.g., drop columns with >50% NaN values)\n",
    "clean_df = drop_nan_columns(df, threshold=0.5)\n",
    "print(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, sequence_length=10):\n",
    "    \"\"\"Prepare data for LSTM model\"\"\"\n",
    "    # Select input features\n",
    "    id_columns = ['date_id', 'time_id', 'symbol_id']\n",
    "    feature_columns = [col for col in df.columns if 'feature_' in col]\n",
    "    input_columns = id_columns + feature_columns\n",
    "    \n",
    "    # Separate input and output\n",
    "    X = df[input_columns].values\n",
    "    y = df['responder_6'].values\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_X = StandardScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(len(X_scaled) - sequence_length):\n",
    "        X_sequences.append(X_scaled[i:(i + sequence_length)])\n",
    "        y_sequences.append(y[i + sequence_length])\n",
    "    \n",
    "    return np.array(X_sequences), np.array(y_sequences), scaler_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for time series data\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM Model\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=256, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer,schedular, num_epochs, device):\n",
    "    \"\"\"Train the model with progress bars\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    # Create epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc='Training Progress', position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Create batch progress bar\n",
    "        batch_pbar = tqdm(\n",
    "            train_loader, \n",
    "            desc=f'Epoch {epoch+1}/{num_epochs}',\n",
    "            leave=False,\n",
    "            position=1\n",
    "        )\n",
    "        \n",
    "        for batch_X, batch_y in batch_pbar:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            batch_pbar.set_postfix({\n",
    "                    'batch_loss': f'{loss.item():.4f}',\n",
    "                    'lr': f'{current_lr:.6f}'\n",
    "                    })\n",
    "        \n",
    "        # Average training loss\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(\n",
    "                val_loader,\n",
    "                desc='Validation',\n",
    "                leave=False,\n",
    "                position=1\n",
    "            )\n",
    "            \n",
    "            for batch_X, batch_y in val_pbar:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Update validation progress bar\n",
    "                val_pbar.set_postfix({'val_batch_loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Average losses\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'val_loss': f'{val_loss:.4f}',\n",
    "            'best_val_loss': f'{best_val_loss:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Close progress bars\n",
    "    epoch_pbar.close()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "    \n",
    "# Parameters\n",
    "SEQUENCE_LENGTH = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "# Prepare data\n",
    "X_sequences, y_sequences, scaler_X = prepare_data(df, SEQUENCE_LENGTH)\n",
    "    \n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X_sequences, \n",
    "y_sequences, \n",
    "test_size=0.2, \n",
    "random_state=42,\n",
    "shuffle=False\n",
    "    )\n",
    "    \n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "# Initialize model\n",
    "input_size = X_sequences.shape[2]  # Number of features\n",
    "model = LSTMModel(input_size=input_size).to(DEVICE)\n",
    "    \n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "# Train model\n",
    "model, train_losses, val_losses = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    EPOCHS, \n",
    "    DEVICE\n",
    "    )\n",
    "    \n",
    "# Evaluate model\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_actual = []\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(DEVICE)\n",
    "        outputs = model(batch_X)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "        test_actual.extend(batch_y.numpy())\n",
    "    \n",
    "# Calculate MAE\n",
    "mae = np.mean(np.abs(np.array(test_predictions) - np.array(test_actual)))\n",
    "print(f'\\nTest MAE: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_actual = []\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(DEVICE)\n",
    "        outputs = model(batch_X)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "        test_actual.extend(batch_y.numpy())\n",
    "    \n",
    "# Calculate metrics\n",
    "mae = np.mean(np.abs(np.array(test_predictions) - np.array(test_actual)))\n",
    "mse = np.mean((np.array(test_predictions) - np.array(test_actual))**2)\n",
    "rmse = np.sqrt(mse)\n",
    "    \n",
    "print('\\nTest Metrics:')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
